

potrf : mpiexec -np 1 ./tests/testing_dpotrf -c 40 -g 4 -p 1 -q 1 -t 1000 -T 1000 -N 100000 --nruns=2

gemm : mpiexec -np 1 ./tests/testing_dgemm -c 40 -g 4 -p 1 -q 1 -t 1000 -T 1000 -N 100000 --nruns=2

sparse gemm : mpiexec -np 1 ~/irr-gemm-gpu-over-parsec/build/src/testing_dgemm_irrse -P 1 -Q 1 -M 20000 -N 800000 -K 20000 -m 100 -n 100 -k 100

vector multiplication : mpiexec -np 1 ~/dplasma/build/tests/testing_dgemm -c 40 -g 2 -p 1 -q 1 -t 1000  -T 1000 -N 100000 -M 100000 -K 1000 --nruns=2

cuda profiling : ncu --target-processes all -o profile ./tests/testing_dpotrf -c 8 -g 4 -p 1 -q 1 -t 1000 -T 1000 -N 100000 --nruns=2

node_level_tests : mpiexec -np 1 ~/dplasma/build/tests/testing_dgemm -c 40 -g 2 -p 1 -q 1 -t 10 -T 10 -N 100  --nruns=1

mca paremeter : mpirun -np 1 ~/irr-gemm-gpu-over-parsec/build/src/testing_dgemm_irr_sparse -P 1 -Q 1 -M 20000 -N 800000 -K 20000 -m 100 -n 100 -k 100 -- --mca device_cuda_enabled 4

sparse tiling regular at M/m x K/k for A, K/k x N/n for B and M/m x N/n for C


The goal of the algorithm in the irregular sparse GEMM is to avoid being in this situation.
New
9:34
If you evict read-write tiles, it means you'll need them soon back on the GPU for GEMM. You're not finished with them. So your GPU will be thrashing: it'll be evicting pages to make room, then discover that there is still one GEMM to do on that tile, request the tile on the GPU again, which means that it needs to evict another tile, etc... And it means that we have evicted all the read-only tiles that we could, which means we'll need to re-load tiles of B and A... It's bad

##### TERMEDT #########

When configuring PaRSEC use -DPARSEC_PTGPP_FLAGS=--dynamic-termdet
When configuring dplasma use -DPARSEC_PTGPP_FLAGS="--dynamic-termdet"
